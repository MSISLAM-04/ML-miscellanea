{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unbiased estimators\n",
    "\n",
    "## Introduction\n",
    "\n",
    "\n",
    "## Linear regression: the model\n",
    "\n",
    "Suppose we have a dataset consisting of $m$ pairs input/output of the form $\\{(x^{(1)},y^{(1)}),\\dots,(x^{(m)},y^{(m)})\\}\\subset\\mathbb{R}^n\\times\\mathbb{R}$. We call this set the training data. Suppose we are interested in predicting the output value for a future input $x$ using a linear model. We denote by $y$ the real observable output value associated to the input $x$ and $\\widehat{y}$ the predicted value using the linear model. We suppose that the underlying model has the form\n",
    "\n",
    "$$\n",
    "y = \\theta_0 + \\sum_{j=1}^n \\theta_j x_j + \\epsilon,\n",
    "$$\n",
    "\n",
    "where $\\theta = (\\theta_0,\\dots,\\theta_n)\\in\\mathbb{R}^{n+1}$ is a parameters vector and $\\epsilon$ is a random error function. Thus, for each measured data point, we have\n",
    "\n",
    "$$\n",
    "y^{(k)} = \\theta_0 + \\sum_{j=1}^n \\theta_j x^{(k)}_j + \\epsilon^{(k)},\n",
    "$$\n",
    "\n",
    "where $\\epsilon^{(k)}$ are different random variables. We assume that the error functions $\\epsilon$ have the following properties:\n",
    "- $\\mathbb{E}(\\epsilon) = 0 $,\n",
    "- $\\newcommand{\\var}{var} \\var(\\epsilon)=\\sigma^2<\\infty$,\n",
    "- For different measurements, the random variables $\\epsilon^{(k)},\\epsilon^{(j)}$ are uncorrelated.\n",
    "\n",
    "We do not make any assumption about the distribution of $\\epsilon$, although it is usually assumed for many applications that $\\epsilon$ has normal distribution. There are many possible sources of error, and we can think of it arising for instance, as imprecisions in the measurement instruments. Sometimes the stronger assumption of independence of the errors is assumed. The linear model that we will use to make predictions is of the form\n",
    "\n",
    "$$\n",
    "\\widehat{y} = \\widehat\\theta_0 + \\sum_{j=1}^n \\widehat\\theta_j x_j,\n",
    "$$\n",
    "\n",
    "where $\\widehat\\theta = \\left(\\widehat\\theta_0,\\dots,\\widehat\\theta_n\\right)\\in\\mathbb{R}^{n+1}$ is a parameters vector which estimates $\\theta$. For convenience, we stack all the inputs of the training examples as rows of a matrix $X$ which we also augment with a column of ones, and we stack the outputs in a vector $y$. Then we can write $\\widehat y = X\\widehat\\theta$ and similarly $y = X\\theta + \\epsilon$.\n",
    "\n",
    "## Assessing the error\n",
    "\n",
    "The ordinary least squares method estimates the parameters $\\theta$ by $\\widehat\\theta = X(X^TX)^{-1}X^Ty$, yielding a predcitor $\\widehat y (x)= \\widehat \\theta x$.\n",
    "\n",
    "**Proposition:** The choice $\\widehat\\theta = X(X^TX)^{-1}X^Ty$ minimizes \n",
    "\n",
    "$$\n",
    "\\dfrac{1}{2m}\\sum_{k=1}^m \\left(y^{k}-\\widehat y^{k}\\right)^2.\n",
    "$$\n",
    "\n",
    "**Proof:** Note that the expression above can be written in matrix form as\n",
    "\n",
    "$$\n",
    "\\dfrac{1}{2m}(X\\theta-y)^T(X\\theta-y).\n",
    "$$\n",
    "\n",
    "With a bit of effort, it can be proved that\n",
    "\n",
    "$$\n",
    "\\nabla \\left(\\dfrac{1}{2m}(X\\theta-y)^T(X\\theta-y) \\right) = \\dfrac{1}{m}(X^TX\\theta-X^Ty).\n",
    "$$\n",
    "\n",
    "Now, for a function to have a minimum at $\\widehat\\theta$, it is necessary that its gradient vanishes. Thus, if $\\widehat\\theta$ minimizes $MSE$, we need\n",
    "\n",
    "$$\n",
    "X^TX\\widehat\\theta-X^Ty=0\n",
    "$$\n",
    "\n",
    "and hence $\\widehat\\theta = (X^TX)^{-1}X^Ty$. The existence of a minimum follows from the fact that the quadratic form $(X\\theta-y)^T(X\\theta-y)$ is positive semi-definite. More details on this can be seen [here](https://stats.stackexchange.com/questions/63143/question-about-a-normal-equation-proof) and [here](https://web.stanford.edu/~mrosenfe/soc_meth_proj3/matrix_OLS_NYU_notes.pdf).$\\spadesuit$\n",
    "\n",
    "\n",
    "\n",
    "**Remark:** the proposition above tells us that the OLS minimizes the mean squared distance from the predictions to the actual values of the observed data.\n",
    "\n",
    "Recall that the **mean squared error** is defined by\n",
    "\n",
    "$$\n",
    "MSE = \\mathbb{E}(y-\\widehat y)^2.\n",
    "$$\n",
    "\n",
    "One of the reasons to choose the MSE as a measure of how fitting to our data the model is, is the bias/variance decomposition, as seen in a [previous notebook](https://github.com/felperez/ML-miscellanea/blob/master/Bias%20vs%20variance.ipynb). This gives the MSE an intuitive interpretation, and makes the model selection a slightly simpler task.\n",
    "\n",
    "Note: there ara many arguments in favor of this choice as a measure of the errors on the predictions. There are other different choices for such function, such as the absolute deviations. An interesting discussion can be found [here](https://stats.stackexchange.com/questions/147001/is-minimizing-squared-error-equivalent-to-minimizing-absolute-error-why-squared). Another good source of information can be found [in these notes](https://web.stanford.edu/~mrosenfe/soc_meth_proj3/matrix_OLS_NYU_notes.pdf). Recall the decomposition\n",
    "\n",
    "$$\n",
    "MSE = \\sigma^2 + \\text{var}(\\widehat f) + \\text{bias}^2(\\widehat f),\n",
    "$$\n",
    "\n",
    "where $\\text{var}(\\widehat f) = \\mathbb{E}(\\widehat f^2)-(\\mathbb{E}\\widehat f)^2$ and $\\text{bias}(\\widehat f)=\\mathbb{E}(\\widehat f - f)$.\n",
    "\n",
    "**Proposition:** the model $\\widehat f (x)=\\widehat\\theta x $ has zero bias.\n",
    "\n",
    "**Proof:** this follows from the observation that $\\mathbb{E}(\\widehat\\theta)=\\theta$. In fact, note that\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbb{E}(\\widehat \\theta) &=\\mathbb{E}((X^TX)^{-1}X^Ty) \\\\\n",
    "&= \\mathbb{E}((X^TX)^{-1}(X^Ty)) \\\\\n",
    "&= \\mathbb{E}((X^TX)^{-1}(X^TX\\theta+X^T\\epsilon)) \\\\\n",
    "&= \\theta + \\mathbb{E}((X^TX)^{-1}X^T\\epsilon) \\\\\n",
    "& = \\theta\n",
    "\\end{align*}\n",
    "\n",
    "since the matrices involving $X$ are non random.$\\spadesuit$\n",
    "\n",
    "The previous proposition says that if the data is in fact given by a linear model, the ordinary least squares estimator has zero bias, and thus its contribution to the MSE only comes from the variance (as well as the noise term). This might seem like the best possible scenario, but in fact, as we will see in future notebooks, this is usually not the case, as some biased estimators can achieve overall smaller MSE by having a much smaller variance. This concerns particularly the non linear models fitted by using enlarged feature spaces.\n",
    "\n",
    "It is possible to prove that among the unbiased linear estimators, the one provided by the Ordinary Least Squares achieves the minimal variance. This result is known as the [Gauss-Markov theorem](https://en.wikipedia.org/wiki/Gaussâ€“Markov_theorem)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
